\chapter{Introducci\'on}
\label{cap:introduccion}

\section{Antecedentes y motivaci\'on}
\label{intro:motivacion}
El \textit{Atacama Large Millimeter/Submillimiter Array} es un instrumento revolucionario en su concepto científico, su diseño de ingeniería y su organización como un esfuerzo científico global. Construido en el desierto de Atacama de Chile septentrional a una altitud de 5.000 metros sobre el nivel del mar en el llano Chajnantor, ALMA cuenta con 66 antenas de alta precisión funcionando juntas en longitudes de onda milimétricas y submilimétricas. Gracias a su alta resolución y sensibilidad, ALMA abre una “ventana” totalmente nueva al Universo, permitiendo a los científicos desenmarañar misterios astronómicos importantes y de muchos años, explorando nuestros orígenes cósmicos \citep{alma}.

ALMA captura señales del cielo con dos o más antenas combinadas con el fin de analizar  la señal y así obtener información acerca de la fuente que emite dicha señal (ya sea una estrella, planeta, galaxia u otro objeto astronómico). De esta forma, combinando las ondas de radio recolectadas por muchas antenas es posible construir una imagen. Estas imágenes son comparables con un hipotético telescopio gigante o una antena de 14.000 metros de diámetro. Como la construcción y operación de una antena de tal tamaño es técnicamente imposible, lo factible es construir muchas antenas pequeñas y combinar su salida \citep{howalma}.

Sin embargo, este problema no es tan simple como parece, puesto que para la reconstrucción de las imágenes se necesitan algoritmos especializados. Actualmente existen dos grandes tipos de algoritmos: aquellos basados en transformaciones lineales y aquellos con un soporte Bayesiano, no lineal.

Estos últimos tienen una base matemática sólida, pero fueron dejados de lado por su pobre rendimiento computacional. Es por ello que gracias al desarrollo del hardware, especialmente de las GPU, se pretende acelerar estos algoritmos para tener imágenes de mejor calidad en un tiempo menor al que se obtiene con soluciones secuenciales.

\section{Descripci\'on del problema}
\label{intro:problema}

\subsection{Estado del arte}
\label{sec:estadodelarte}


La reconstrucción de imágenes usando radiointerferómetros que captan una señal del cielo a medida que rota la tierra es llamado un problema \textit{ill-posed} \citep{chen}, debido a la irregularidad de los muestreos Nyquist en el dominio de Fourier. Estos muestreos resultan en \textit{aliasing} o sobreposición de ondas en la imagen debido a los altos lóbulos laterales en la respuesta del \textit{array} de antenas. Es importante entender que obtener imágenes de alta calidad depende tanto de contar con los mejores equipos como con las mejores técnicas de reconstrucción. En ese sentido, cada generación de radiotelescopios involucra un esfuerzo en el desarrollo de \textit{hardware}. Sin embargo, para explotar las capacidades de ese \textit{hardware} se requiere una constante mejora en los enfoques de reconstrucción de imágenes para poder igualar la sensibilidad del receptor.

Durante los últimos 40 años, múltiples técnicas de deconvolución han sido desarrolladas para resolver el problema mencionado anteriormente. 

La idea básica detrás de un algoritmo de deconvolución es explotar un conocimiento \textit{a-priori} de la imagen. El primer algoritmo, y la más popular de estas técnicas en el campo de la astronomía, es el método CLEAN propuesto por Högbom \citep{CLEAN}. Este método se basa en aproximar la imagen usando lo que se denomina \textit{dirty image}. La \textit{dirty image} se muestra en la ecuación \ref{eq:dirtyimage} y es la imagen obtenida luego de aplicar la transformada inversa de Fourier sobre el conjunto de visibilidades $V$ multiplicada por la función de muestreo $S(u,v)$, tal que $S(u,v)=1$ donde exista una visibilidad y $S(u,v)=0$ si no existe. Las coordenadas $(u,v)$ representan las coordenadas de las mediciones en el plano de Fourier esparcidos de forma irregular. Por otra parte los puntos $(x,y)$ representan las coordenadas planas en la imagen escalada. 

Por lo tanto, CLEAN asume que la imagen es una colección de fuentes puntuales e iterativamente remueve fuentes puntuales para crear una imagen de residuos.

Sin embargo, este método posee tanto deficiencias técnicas como matemáticas. Esto debido a que CLEAN no ofrece una alternativa de regularización del problema y tampoco una interpretación estadística del resultado que entrega \citep{libroAstro2}. Además, CLEAN no tiene una convergencia definida por lo que debe iterarse según la apreciación que el usuario tiene de la imagen. Además, el problema de la reconstrucción de una imagen consiste en seleccionar una solución de muchas posibles y CLEAN sólo selecciona una probable imagen de un conjunto de soluciones factibles.
 
Otra técnica de deconvolución importante es el método de máxima entropía (MEM), que consiste en seleccionar una imagen que se ajuste a las visibilidades observadas dentro de un nivel de ruido y cuyos píxeles satisfagan la restricción de máxima entropía.

Las primeras presentaciones de MEM fueron hechas por \citep{FRIEDEN:72, mem2, daddario, themaxen, skilling1984} en donde la técnica fue modificada a lo largo de los años de tal forma de probar diversas funciones y formas de entropía. Por ejemplo, \citep{FRIEDEN:72} y \citep{themaxen} consideran distribuciones de intensidad dispersas aleatoriamente en el cielo. Es por ello que la entropía que se utiliza en estos trabajo es de la forma: $\sum_{i}\frac{I_{i}}{I_{s}}\log(\frac{I_{i}}{I_{s}})$, donde $I_{i}=I(x_{i}, y_{i})$ e $I_{s}=\sum_{i}I_{i}$, y en donde además \citep{FRIEDEN:72} usa el método de Newton-Raphson para llevar acabo la maximización del problema, sin embargo queda claro que es sólo exitoso en problemas pequeños. Por otra parte, \citep{mem2} y \citep{daddario} se utiliza una entropía de la forma $\sum_{i}\log I_{i}$. Finalmente, en \citep{skilling1984} se resume el método y se nombran distintos tipos de algoritmos de optimización, concluyendo con un método que propone deshacerse del uso del penalizador explícito $\lambda$ construyendo así un enfoque general del método.

%Acá faltan las contribuciones de MEM

%Por otro lado, tomando en cuenta que MEM es un problema no lineal, Cornwell y Evans \citep{smemda} modifican el algoritmo de minimización de MEM usando Newton-Raphson.



%En ese sentido, esto es un problema, puesto que los astrónomos desean tener los datos en un tiempo acotado para poder observarlos, si bien CLEAN cumple con esto, se llega a la pregunta de ¿Qué tan cierto es lo que se está observando? Esto puede responderse sólo con un enfoque matemático y estadístico que puede ser llevado a cabo por el algoritmo iterativo MEM.



Más allá de estas dos técnicas, existen otros algoritmos que no son tan famosos en las astronomía. Por ejemplo, han sido desarrollada una técnica de mínimos cuadrados globales no-negativos \textit{Global nonnegative least squares}(LS) propuesta por Briggs \citep{briggs1995}. Esto es una técnica basada en matrices paramétricas como la de mínima varianza (\textit{LS minimum variance imaging}, LS-MVI) \citep{Lesheradioastronomical}, y por último una técnica de máxima verosimilitud \textit{Maximum likelihood} \citep{BenDavid:2008ff}.

Por otro lado, las tarjetas gráficas, o GPU, son \textit{hardware} que durante los últimos años han pasado de ser de propósito particular, es decir, sólo útil para juegos, películas 3D e interfaces gráficas, a ser de propósito general (GPGPU). Esto gracias a que en los últimos años se ha visto una explosión en el interés científico por aprovechar las capacidades de estos \textit{chips} para llevar a cabo aplicaciones y cómputos de propósito general. Así es como las GPGPU han sido utilizadas para llevar a cabo simulaciones, aplicaciones de visión por computador, procesamiento y análisis de señales, procesamiento de imágenes y minería de datos \citep{Owens:2007:ASO}.


Recientemente, se propuso un enfoque que busca realizar inferencias Bayesianas para observaciones radioastronómicas (BIRO) \citep{BIRO}. Este enfoque lleva a cabo una inferencia Bayesiana para muestrear un conjunto de parámetros representando un modelo del cielo para proponer las visibilidades que mejor se ajusten a éste. Este trabajo parametriza la imagen en un conjunto reducido de parámetros, y no reconstruye la imagen en sí, sino un modelo de imagen, restringido a puntos y funciones Gaussianas. Además se ha propuesto Montblanc \citep{montblanc}, una implementación en GPU en apoyo a BIRO y que permite paralelizar la proposición de visibilidades.

\subsection{Definición del problema}
El radio observatorio ALMA \citep{alma2} entrega  datos observacionales  en  el  espectro  de  radio  frecuencias con  una  resolución  nunca  antes  lograda.  Para  una 
observación  dada,  este  instrumento  recoge  un  subconjunto  discreto y disperso (no regular) de  la  transformada  de  Fourier  de  la imagen. La cantidad de muestras (visibilidades) depende del objeto que se esté observando y del tiempo de observación. La Tabla \ref{tab:data} muestra el número de visibilidades observadas de el set de datos HLTau muestrado en la banda de frecuencia 6 y su respectivo valor en bytes. Este último número es calculado multiplicando el número de visibilidades observadas por la suma bytes de las variables utilizadas, tal como, las coordenadas de las visibilidades en el plano de Fourier, las visibilidades que contienen una parte real e imaginaria y la varianza asociada a éstas. Cada una de estas variables es considerada un \textit{double} por lo cual cada una tiene un valor de 8 bytes. Cabe destacar que si bien el número de bytes no es tan alto, para cada objeto observado se muestrean diferentes características como las moléculas de monóxido de carbono, las moléculas de bicarbonato, entre otras. Además, también es importante tomar en cuenta que los objetos se muestrean en distintas bandas de frecuencias. Sin embargo, el problema de reconstruir la imagen no pasa simplemente por usar la transformada inversa del conjunto de puntos. Pues como se dijo anteriormente, los puntos recolectados no coiciden con una grilla regular. Este problema es conocido en matemáticas como \textit{“Fourier Synthesis”} \citep{marechal} y sorprendentemente aún se encuentra como una floreciente línea de investigación en el campo del análisis armónico \citep{pablo}.


%\begin{table}[h!]
%\centering
%\caption{Set de datos y visibilidades}
%\label{tab:data}
%\begin{tabular}{|l|l|l|}
%\hline
%\rowcolor[HTML]{EFEFEF} 
%Set de datos   & Número de visibilidades & Bytes       \\ \hline
%HD142527       & 78.375                  & 3.135.000   \\ \hline
%CO(6-5)        & 120.982                 & 4.839.280   \\ \hline
 %HLTau - Band 6 & 5.343.914               & 213.756.560 \\ \hline
%\end{tabular}
%\end{table}

\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
\begin{table}[h!]
\centering
\ra{1.2}
\caption{Datos HLTau}
\label{tab:data}
\begin{tabular}{@{}rcrrcr@{}} 
\toprule
 \multicolumn{1}{c}{{\bf Set de datos}} & \phantom{a} & \multicolumn{2}{c}{{\bf Número de visibilidades}}  & \multicolumn{1}{c}{{\bf Bytes}} \\
 \midrule
 Band 3   && - &&  -\\
 Band 6   && 5.343.914 &&  213.756.560\\    
 Band 7   && - &&  -\\
 \toprule
\end{tabular}
\end{table}

%Poner illposedness
%\subsubsection{CLEAN}

Son variados los métodos que permiten reconstruir una imagen a partir de muestras dispersas y no regulares en el espacio Fourier. Entre ellos se encuentran los conocidos métodos CLEAN \citep{CLEAN} y MEM \citep{MEM}, en donde el primero es un algoritmo basado en transformaciones lineales y el segundo basado en la teoría Bayesiana. Ambos son conocidos particularmente en el campo de la astronomía.


El algoritmo CLEAN provee una solución basada en representar la imagen mediante un conjunto de fuentes puntuales. La imagen final, mejor conocida como la imagen CLEAN, es la suma de todos los componentes de los puntos convolucionados con el \textit{CLEAN beam}, esto es, la imagen de una función Gaussiana, con el fin de restar importancia a las frecuencias espaciales más altas que suelen ser falsamente extrapoladas \citep{libroAstro2}. Finalmente, CLEAN es un procedimiento cuyo objetivo es generar una imagen de residuos con variaciones homogéneas y similares al ruido térmico presente en los datos. 

Como se menciona en la sección \ref{sec:estadodelarte}, CLEAN posee deficiencias técnicas y matemáticas, a pesar de su éxito y su uso actual en ALMA.

Sin embargo, este método sigue siendo utilizado como una herramienta diaria por la comunidad astronómica y es preferido de acuerdo a su eficiencia computacional, pese a que los algoritmos Bayesianos tales como MEM, generan mejores imágenes y gozan de mayor sustento estadístico. Sin embargo, estos últimos no se utilizan porque se desconocen sus propiedades estadísticas, debido a que no existe una estrategia de paralelización que provea soluciones de forma rápida y que permita estudiar el rendimiento a nivel de calidad de imágenes del algoritmo.

En ese sentido la pregunta de investigación que se desea responder es: ¿El algoritmo MEM produce imágenes de mejor calidad que CLEAN?

También es importante mencionar que MEM tiene un enfoque Bayesiano que tiene como resultado una función no lineal. Para minimizar este tipo de funciones existen distintos tipos de métodos de optimización, como por ejemplo, el método de gradiente conjugado \citep{ncg}. Por lo tanto, el desafío técnico de este trabajo es diseñar una estrategia de paralelización en una arquitectura SIMT para este problema.



\section{Soluci\'on propuesta}
\label{intro:solucion}
\subsection{Características de la solución}
\label{subsec:caracteristicas}
Se pretende llevar a cabo el desarrollo de un algoritmo de reconstrucción basado en el principio de máxima entropía, aplicando una estrategia de paralelización en múltiples GPU y usando la plataforma de computación paralela CUDA, para luego caracterizar el algoritmo a nivel de calidad de imagen.

El método de máxima entropía (MEM) encuentra la imagen que mejor se ajusta a los datos, dentro de un nivel de ruido y que maximiza la entropía $S$. Esto es llevado a cabo minimizando la función:

\begin{equation}
\Phi = \chi^{2} + \lambda S
\label{eq:firstchi}
\end{equation}

Donde en el caso de que los datos provengan de un interferómetro, se tiene que:

\begin{equation}
\chi^{2} = \sum_{k=1}^{Z} \frac{|V^m_{k}-V^o_{k}|^{2}}{\sigma^{2}_{k}}
\end{equation}

Donde la sumatoria recorre todas las visibilidades y el numerador de la fracción representa la diferencia entre las visibilidades modelo, creadas luego de realizar una transformada de Fourier a la imagen, llevar las visibilidades observadas a una grilla regular y luego realizar una interpolación bilineal; y las visibilidades observadas. Así, esta diferencia es llamada visibilidad residuo y se debe calcular el módulo del complejo para luego elevarlo al cuadrado. Por otra parte, $\sigma_{k}$ es la varianza de cada visibilidad en función del ruido termal de las antenas.

La entropía $S$ es un término regularizador en un problema inverso, donde se tienen más parámetros libres que datos. Si bien existen distintas formulaciones para $S$, la solución a presentar usa una entropía de forma $S = \sum_{i}^{n} I_{i}log(\frac{I_{i}}{G})$ donde $\{I_{i}\}_{i=1}^{n}$ es el valor de intensidad de la imagen en el píxel $i$ y $G$ es el menor valor de intensidad que puede tomar la imagen. Además, $\lambda$ actúa como un penalizador y tiene un rol similar a los multiplicadores de Lagrange.

 
Es importante destacar que la Ecuación \ref{eq:firstchi} es no lineal y se debe minimizar con algoritmos apropiados agregando una restricción de positividad, como por ejemplo, el método de gradiente conjugado Polak-Ribiere \citep{polak}.

Así, MEM produce una imagen positiva en un rango definido de píxeles haciendo que la imagen final sea suave y que tenga una super-resolución en brillos y objetos aislados, sin embargo, al ser un problema no lineal su costo en tiempo computacional es muy alto \citep{libroAstro2}, es por ello que una o muchas GPUs calcularán $\Phi$ y $\nabla\Phi$ para cada frecuencia, para luego hacer una reducción en una única GPU si es que se utilizan múltiples GPU. Para ello, se hace necesario usar estructuras de datos apropiadas para almacenar valores complejos e imágenes.

\subsection{Propósitos de la solución}
El propósito de la solución es caracterizar el algoritmo MEM a nivel de calidad de imagen con el fin de darle a conocer a la comunidad astronómica sus propiedades, de manera que se transforme en una alternativa real para obtener imágenes de gran resolución y poco ruido en un tiempo acotado y de manera práctica.


\section{Objetivos y alcance del proyecto}
\label{intro:objetivos}

\subsection{Objetivo general}
El objetivo general del proyecto es diseñar y caracterizar un algoritmo de alto rendimiento basado en el método de máxima entropía (MEM), es decir, medir parámetros de rendimiento, resolución y nivel de ruido de las imágenes reconstruidas a partir de datos sintéticos para proveer a la comunidad astronómica una solución de reconstrucción de imágenes que sea de uso práctico y que además genere imágenes de mejor calidad que los algoritmos usados actualmente.
.

\subsection{Objetivos espec\'ificos}

\begin{enumerate}
\item Formular matemáticamente el método MEM.
\item Diseñar una estrategia de paralelización en múltiples GPU para el método.
\item Llevar a cabo una implementación en múltiples GPU mediante la plataforma de computación paralela \textit{CUDA}.
\item Evaluar el rendimiento computacional, comparando los tiempos de ejecución de la solución a implementar en multi-GPU y la versión multi-hebreada de MEM existente.
\item Evaluar la calidad de la imagen obtenida, comparando la imagen obtenida con la imagen CLEAN respectiva que usan los astrónomos actualmente.
\end{enumerate}

\subsection{Alcances}
La solución adoptada es implementada sólo mediante la plataforma de computación paralela CUDA y el lenguaje de programación C. Además, solo se adopta una estrategia de paralelización  para el algoritmo MEM y las pruebas de rendimiento computacional sólo son realizadas en un cluster de GPUs. Por otra parte, los resultados obtenidos serán comparados solamente con el método de reconstrucción de imágenes llamado CLEAN, esto debido a que es el método más utilizado por los astrónomos actualmente. Por último, el formato de la imagen de salida debe ser en formato FITS. 


\section{Metodolog\'ia y herramientas utilizadas}
\label{intro:metodologia}

\subsection{Metodolog\'ia}
El trabajo a realizar posee una etapa inicial de investigación, en la cual por medio de la literatura, la observación y la experimentación se debe estudiar el método MEM  para luego buscar diseños de paralelización adecuados a éste. Bajo dicha perspectiva el método científico es útil para guiar la totalidad del trabajo. Sin embargo, como se menciona anteriormente, una vez que se haya obtenido el diseño, se debe desarrollar un software, por lo que se utilizarán reglas de la metodología \textit{Extreme Programming}(XP) que ya han sido probadas en el ámbito de la investigación científica \citep{xp}. Las etapas de esta metodología son:

\begin{enumerate}
\item \textbf{Feedback}: Se toma en cada iteración  un compromiso serio por la entrega de software que  se trabaja. Se muestra el software temprano y con frecuencia luego se escucha con atención al cliente y hacen los cambios necesarios.

\item \textbf{Diseño incremental}: Se debe trabajar por un cierto tiempo, por ejemplo, dos semanas (iteración), y al final de éstas debe entregarse un software funcionando.

\item \textbf{Programación en parejas (\textit{Pair programming})}: Los programadores trabajan en parejas. Todo el código es desarrollado por dos programadores (alumno y profesor guía en este caso).

\item \textbf{Reuniones diarias}: El propósito de esto es tener una reunión diaria para comunicar problemas, soluciones y promover la concentración del equipo.

\item \textbf{Pruebas}: Por cada funcionalidad nueva, se deben realizar pruebas unitarias, pruebas de aceptación, pruebas de humo, de stress y rendimiento.

\item \textbf{Integración continua}: Se deben automatizar las pruebas anteriormente mencionadas, para que cada vez que se agreguen nuevas funcionalidades se asegure que todas siguen funcionando.
\end{enumerate}

\subsection{Herramientas de desarrollo}
A continuación se presentan las herramientas que se utilizarán para el desarrollo del
proyecto.


Para el desarrollo de prototipos y pruebas de concepto se utiliza:
\begin{enumerate}
\item Sistema Operativo Ubuntu 14.04.
\item Matlab 2015a.
\end{enumerate}
Para el desarrollo, compilación y ejecución de las implementaciones paralelas se utilizara:
\begin{enumerate}
\item Sistema Operativo Ubuntu 14.04.
\item CUDA 7.0.
\end{enumerate}

%Para la escritura del documento se utilizararon las siguientes herramientas:
%\begin{enumerate}
%\item Sistema Operativo Ubuntu 14.04.
%\item TexMaker.
%\end{enumerate}

\subsection{Ambiente de desarrollo}

En cuanto al ambiente de desarrollo, cabe destacar que existe un ambiente de pruebas y uno de produccción.

Las pruebas de rendimiento y producción serán realizadas un cluster de GPUs con las siguientes características:


\begin{enumerate}
\item 2x 10 Core Intel Xeon E5-2640 V2 2GHz
\item 256 GB de RAM
\item 4 Tarjetas de video NVIDIA Tesla K80
\end{enumerate}

Además son utilizados los recursos computacionales personales del estudiante, los cuales son:

\begin{enumerate}
\item Un computador de escritorio con:
\begin{itemize}
\item 8 GB de RAM.
\item Procesador Intel i7-3770 3.40GHz x 8. 
\item Tarjeta de video NVIDIA GeForce GTX 750 (1 GB).
\end{itemize}
\item Un computador portátil ASUS 450LN con:
\begin{itemize}
\item 8 GB de RAM DDR3.
\item Procesador Intel i7-4500U (1800 MHz-3000 MHz)
\item Tarjeta de video híbrida Intel GMA HD Graphics 4400 \& NVIDIA GeForce 840M (2 GB)
\end{itemize}
\end{enumerate}

\section{Organizaci\'on del documento}
\label{intro:organizacion}
\textcolor{red}{falta organizacion del documento}
